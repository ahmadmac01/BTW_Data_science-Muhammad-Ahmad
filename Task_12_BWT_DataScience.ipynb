{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "In machine learning, evaluating the performance of a model is crucial to understanding how well it generalizes to new data. Three common metrics used for classification tasks are precision, recall, and accuracy.\n",
        "\n",
        "### 1. Accuracy\n",
        "Definition: Accuracy is the ratio of correctly predicted instances to the total instances in the dataset.\n",
        "Formula:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "Accuracy = True Positives + True Negatives / Total Instances\n",
        "```\n",
        "\n",
        "### Usage: Accuracy is useful when the classes are balanced, meaning the number of instances of each class is roughly equal.\n",
        "\n",
        "Example: In a spam detection system, if out of 100 emails, 90 are correctly classified (either as spam or not spam), the accuracy is 90%.\n",
        "\n",
        "2. Precision\n",
        "Definition: Precision is the ratio of correctly predicted positive observations to the total predicted positives.\n",
        "\n",
        "####Usage: Precision is important when the cost of a false positive is high. For example, in a spam detection system, a high precision means that most of the emails classified as spam are actually spam.\n",
        "\n",
        "Example: If the model predicts 30 emails as spam and 25 of them are actually spam, the precision is\n",
        "25\n",
        "30\n",
        "=\n",
        "0.83\n",
        "30\n",
        "25\n",
        "​\n",
        " =0.83.\n",
        "\n",
        "\n",
        "3. Recall\n",
        "Definition: Recall (or Sensitivity) is the ratio of correctly predicted positive observations to the all observations in the actual class.\n",
        "Formula:\n",
        "\n",
        "Recall\n",
        "=\n",
        "True Positives\n",
        "True Positives\n",
        "+\n",
        "False Negatives\n",
        "Recall=\n",
        "True Positives+False Negatives\n",
        "True Positives\n",
        "​\n",
        "\n",
        "\n",
        "#### Usage: Recall is important when the cost of a false negative is high. For example, in a spam detection system, a high recall means that most of the actual spam emails are being correctly identified as spam.\n",
        "\n",
        "Example: If there are 50 spam emails and the model correctly identifies 40 of them, the recall is\n",
        "40\n",
        "50\n",
        "=\n",
        "0.8\n",
        "50\n",
        "40\n",
        "​\n",
        " =0.8.\n",
        "\n",
        "Example Scenario\n",
        "True Positives (TP): Emails correctly identified as spam.\n",
        "True Negatives (TN): Emails correctly identified as not spam.\n",
        "False Positives (FP): Emails incorrectly identified as spam.\n",
        "False Negatives (FN): Emails incorrectly identified as not spam."
      ],
      "metadata": {
        "id": "DHtoEvnOru67"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Choosing the Right Metric\n",
        "The choice between precision, recall, and accuracy depends on the specific problem and its context\n",
        "\n",
        "* Accuracy is a good measure when classes are balanced and there is a similar cost for false positives and false negatives.\n",
        "* Precision is preferred when the cost of false positives is high, such as in spam detection or fraud detection.\n",
        "* Recall is important when the cost of false negatives is high, such as in medical diagnosis or fault detection systems."
      ],
      "metadata": {
        "id": "UkBYyZFYtDEk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "NwoyBB5Sq08I"
      },
      "outputs": [],
      "source": [
        "y_pred = [1, 0, 1, 1, 0, 1, 0, 0, 1, 0]\n",
        "y_true = [1, 0, 1, 0, 0, 1, 0, 1, 1, 0]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix\n",
        "\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "precision = precision_score(y_true, y_pred)\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "\n",
        "recall = recall_score(y_true, y_pred)\n",
        "print(f\"Recall: {recall:.2f}\")\n",
        "\n",
        "conf_matrix = confusion_matrix(y_true, y_pred)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uB44jkn8tSXX",
        "outputId": "292592e5-a8c0-4c57-91fd-e694a8b46f63"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.80\n",
            "Precision: 0.80\n",
            "Recall: 0.80\n",
            "Confusion Matrix:\n",
            "[[4 1]\n",
            " [1 4]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NY4lPKPwtvYl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}